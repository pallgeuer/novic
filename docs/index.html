<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Given an image and nothing else, NOVIC can generate an accurate fine-grained textual classification label in real-time, with coverage of the vast majority of the English language.">
  <meta name="keywords" content="NOVIC, AI, deep learning, open vocabulary, image classification">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NOVIC: Unconstrained Open Vocabulary Image Classification</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-5JDV6QQD13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-5JDV6QQD13', { cookie_domain: 'pallgeuer.github.io' });
  </script>

  <link rel="icon" href="./static/images/icon.png">

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/bulma-no-dark-mode.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="#">
        <span class="icon">
          <i class="fas fa-home"></i>
        </span>
      </a>
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          Other Work
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/pallgeuer/novic">
            NOVIC
          </a>
          <a class="navbar-item" href="https://github.com/pallgeuer/object_noun_dictionary">
            Object Noun Dict
          </a>
          <a class="navbar-item" href="https://github.com/pallgeuer/chatty_robots">
            Chatty Robots
          </a>
          <a class="navbar-item" href="https://github.com/pallgeuer/gpt_batch_api">
            GPT Batch API
          </a>
          <a class="navbar-item" href="https://github.com/pallgeuer/model_testbed">
            Model Testbed
          </a>
          <a class="navbar-item" href="https://github.com/pallgeuer/DeepStack">
            DeepStack
          </a>
          <a class="navbar-item" href="https://github.com/AIS-Bonn/humanoid_op_ros">
            Humanoid Soccer
          </a>
          <a class="navbar-item" href="https://arxiv.org/abs/2011.10339">
            Bipedal Walking
          </a>
          <a class="navbar-item" href="https://github.com/AIS-Bonn/attitude_estimator">
            Attitude Estimator
          </a>
          <a class="navbar-item" href="https://github.com/AIS-Bonn/rot_conv_lib">
            3D Rotations
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title publication-title">NOVIC: Unconstrained Open Vocabulary Image Classification: Zero-Shot Transfer from Text to Image via CLIP Inversion</h1>
          <div class="publication-authors">
            <span class="author-block"><a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/allgeuer.html"><b>Philipp Allgeuer</b></a>,</span>
            <span class="author-block"><a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/ahrens.html">Kyra Ahrens</a>,</span>
            <span class="author-block"><a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/wermter.html">Stefan Wermter</a></span>
          </div>
          <div class="publication-authors">
            <span class="author-block">University of Hamburg, Knowledge Technology Group</span>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2407.11211"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/spaces/pallgeuer/novic"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-rocket"></i>
                  </span>
                  <span>Demo</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/pallgeuer/novic"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://paperswithcode.com/dataset/ovic-datasets"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Datasets</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=vyR2QHUH9NY"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Presentation</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://paperswithcode.com/paper/unconstrained-open-vocabulary-image"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-code"></i>
                  </span>
                  <span>Papers With Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="#bibtex"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-quote-left"></i>
                  </span>
                  <span>Cite</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        Given an image and nothing else, NOVIC can generate an accurate fine-grained textual classification label in real-time, with coverage of the vast majority of the English language.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title">NOVIC is...</h2>
        <div class="content">
          <div class="adv-grid">
            <div class="adv-card">
              <b class="keyword">Generative</b><br>
              Image classifications are produced directly as free-form text
            </div>
            <div class="adv-card">
              <b class="keyword">Prompt-free</b><br>
              Does not require <b>any</b> input list of candidate classifications
            </div>
            <div class="adv-card">
              <b class="keyword">Text-only Trained</b><br>
              No images are used at all during training, <b>only</b> synthetic text!
            </div>
            <div class="adv-card">
              <b class="keyword">Efficient</b><br>
              Trains in 1-3 days on a single RTX A6000 (<b>fast</b> as no images required)
            </div>
            <div class="adv-card">
              <b class="keyword">Real-time</b><br>
              26ms inference time per single image, or 7ms per image if batched
            </div>
            <div class="adv-card">
              <b class="keyword">Open Source</b><br>
              Detailed GitHub repository with commands to reproduce results
            </div>
            <div class="adv-card">
              <b class="keyword">Open Vocabulary</b><br>
              Trained using a full English dictionary of nouns (no core or prioritized vocabulary)
            </div>
            <div class="adv-card">
              <b class="keyword">Scalable</b><br>
              Performance scales well with the performance of the underlying CLIP model
            </div>
            <div class="adv-card">
              <b class="keyword">Reliable</b><br>
              Achieves accurate <b>fine-grained</b> prediction scores up to 87.5% on arbitrary images
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title">Live Demo</h2>
        <div class="content has-text-justified">
          <p>
            You can try out NOVIC yourself on your <b>own images</b> using the <a href="https://huggingface.co/spaces/pallgeuer/novic" target="_blank">Hugging Face Spaces Live Demo</a>! Alternatively, you can easily get started inferencing NOVIC yourself on a local GPU, using the <a href="https://hub.docker.com/r/pallgeuer/novic">pre-built NOVIC Docker image</a> and the associated <a href="https://github.com/pallgeuer/novic#inference-novic-yourself">compact instructions</a>.
          </p>
          <div class="publication-image">
            <a href="https://huggingface.co/spaces/pallgeuer/novic" target="_blank">
              <img class="tight-bottom" src="./static/images/novic_spaces.png" alt="Screenshot of live NOVIC model demo on Hugging Face Spaces">
            </a>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title">Approach</h2>
        <div class="content has-text-justified">
          <img src="./static/images/novic.png" alt="Model training and inference pipeline overview schematic">
          <p>
            <b>NOVIC</b> is an innovative u<b>N</b>constrained <b>O</b>pen <b>V</b>ocabulary <b>I</b>mage <b>C</b>lassifier that uses an autoregressive transformer to generatively output classification labels as language. Leveraging the extensive knowledge of CLIP models, NOVIC harnesses the embedding space to enable zero-shot transfer from pure text to images. Traditional CLIP models, despite their ability for open vocabulary classification, require an exhaustive prompt of potential class labels, restricting their application to images of known content or context. To address this, NOVIC uses an <i>object decoder</i> model that is trained on a large-scale 92M-target dataset of templated object noun sets and LLM-generated captions to always output the object noun in question. This effectively inverts the CLIP text encoder and allows textual object labels to be generated directly from image-derived embedding vectors, without requiring any a priori knowledge of the potential content of an image. NOVIC has been tested on a mix of manually and web-curated datasets, as well as standard image classification benchmarks, and achieves fine-grained prompt-free prediction scores of up to 87.5%, a strong result considering the model must work for any conceivable image and without any contextual clues.
          </p>
          <img src="./static/images/object_decoder.png" alt="Object decoder model architecture overview schematic">
          <p>
            At the heart of the NOVIC architecture is the <i>object decoder</i>, which effectively inverts the CLIP text encoder, and learns to map CLIP embeddings to object noun classification labels in the form of tokenized text. During training, a synthetic text-only dataset is used to train the object decoder to map the CLIP text embeddings corresponding to templated/generated captions to the underlying target object nouns. During inference, zero-shot transfer is used to map CLIP image embeddings (as opposed to text embeddings) to predicted object nouns. The ability of the object decoder to generalize from text embeddings to image embeddings is non-trivial, as there is a huge modality gap between the two types of embeddings (for all CLIP models), with the embeddings in fact occupying two completely disjoint areas of the embedding space, with much gap in-between.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title">Presentation Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/vyR2QHUH9NY?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered" id="bibtex">
      <div class="column is-full-width">
        <h2 class="title">BibTeX</h2>
        <div class="content has-text-justified">
    <pre><code>@InProceedings{allgeuer_novic_2025,
    author    = {Philipp Allgeuer and Kyra Ahrens and Stefan Wermter},
    title     = {Unconstrained Open Vocabulary Image Classification: {Z}ero-Shot Transfer from Text to Image via {CLIP} Inversion},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    year      = {2025},
}

@Misc{github_novic,
    title = {{NOVIC}: {U}nconstrained Open Vocabulary Image Classification},
    author = {Philipp Allgeuer and Kyra Ahrens},
    url = {https://github.com/pallgeuer/novic},
}</code></pre>
        </div>
      </div>
    </div>

  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link external-link" href="https://arxiv.org/abs/2407.11211">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link external-link" href="https://huggingface.co/spaces/pallgeuer/novic">
        <i class="fas fa-rocket"></i>
      </a>
      <a class="icon-link external-link" href="https://github.com/pallgeuer/novic">
        <i class="fab fa-github"></i>
      </a>
      <a class="icon-link external-link" href="https://paperswithcode.com/dataset/ovic-datasets">
        <i class="far fa-images"></i>
      </a>
      <a class="icon-link external-link" href="https://www.youtube.com/watch?v=vyR2QHUH9NY">
        <i class="fab fa-youtube"></i>
      </a>
      <a class="icon-link external-link" href="https://paperswithcode.com/paper/unconstrained-open-vocabulary-image">
        <i class="fas fa-file-code"></i>
      </a>
      <a class="icon-link external-link" href="#bibtex">
        <i class="fas fa-quote-left"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <div class="content has-text-centered">
            <p>
              The website template was adapted from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
