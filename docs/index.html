<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Given an image and nothing else, NOVIC can generate an accurate fine-grained textual classification label in real-time, with coverage of the vast majority of the English language.">
  <meta name="keywords" content="NOVIC, AI, deep learning, open vocabulary, image classification">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NOVIC: Unconstrained Open Vocabulary Image Classification</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-5JDV6QQD13"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-5JDV6QQD13', { cookie_domain: 'pallgeuer.github.io' });
  </script>

  <link rel="icon" href="./static/images/icon.png">

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/bulma-no-dark-mode.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="#">
        <span class="icon">
          <i class="fas fa-home"></i>
        </span>
      </a>
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          Other Work
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/pallgeuer/novic">
            NOVIC
          </a>
          <a class="navbar-item" href="https://github.com/pallgeuer/object_noun_dictionary">
            Object Noun Dict
          </a>
          <a class="navbar-item" href="https://github.com/pallgeuer/chatty_robots">
            Chatty Robots
          </a>
          <a class="navbar-item" href="https://github.com/pallgeuer/gpt_batch_api">
            GPT Batch API
          </a>
          <a class="navbar-item" href="https://github.com/pallgeuer/model_testbed">
            Model Testbed
          </a>
          <a class="navbar-item" href="https://github.com/pallgeuer/DeepStack">
            DeepStack
          </a>
          <a class="navbar-item" href="https://github.com/AIS-Bonn/humanoid_op_ros">
            Humanoid Soccer
          </a>
          <a class="navbar-item" href="https://arxiv.org/abs/2011.10339">
            Bipedal Walking
          </a>
          <a class="navbar-item" href="https://github.com/AIS-Bonn/attitude_estimator">
            Attitude Estimator
          </a>
          <a class="navbar-item" href="https://github.com/AIS-Bonn/rot_conv_lib">
            3D Rotations
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title publication-title">NOVIC: Unconstrained Open Vocabulary Image Classification: Zero-Shot Transfer from Text to Image via CLIP Inversion</h1>
          <div class="publication-authors">
            <span class="author-block"><a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/allgeuer.html"><b>Philipp Allgeuer</b></a>,</span>
            <span class="author-block"><a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/ahrens.html">Kyra Ahrens</a>,</span>
            <span class="author-block"><a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/wermter.html">Stefan Wermter</a></span>
          </div>
          <div class="publication-authors">
            <span class="author-block">University of Hamburg, Knowledge Technology Group</span>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2407.11211"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/spaces/pallgeuer/novic"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-rocket"></i>
                  </span>
                  <span>Demo</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/pallgeuer/novic"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://paperswithcode.com/dataset/ovic-datasets"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Datasets</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=vyR2QHUH9NY"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Presentation</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://paperswithcode.com/paper/unconstrained-open-vocabulary-image"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-code"></i>
                  </span>
                  <span>Papers With Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="#bibtex"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-quote-left"></i>
                  </span>
                  <span>Cite</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser is-light">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        Given an image and nothing else, NOVIC can generate an accurate fine-grained textual classification label in real-time, with coverage of the vast majority of the English language.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title">NOVIC is...</h2>
        <div class="content">
          <div class="adv-grid">
            <div class="adv-card">
              <b class="keyword">Generative</b><br>
              Image classifications are produced directly as <b>free-form text</b>
            </div>
            <div class="adv-card">
              <b class="keyword">Prompt-free</b><br>
              Does not require <b>any</b> input list of candidate classifications
            </div>
            <div class="adv-card">
              <b class="keyword">Text-only Trained</b><br>
              No images are used at all during training, <b>only</b> synthetic text!
            </div>
            <div class="adv-card">
              <b class="keyword">Efficient</b><br>
              Trains in 1-3 days on a single RTX A6000 (<b>fast</b> as no images required)
            </div>
            <div class="adv-card">
              <b class="keyword">Real-time</b><br>
              26ms inference time per single image, or 7ms per image if batched
            </div>
            <div class="adv-card">
              <b class="keyword">Open Source</b><br>
              Detailed GitHub repository with commands to reproduce results
            </div>
            <div class="adv-card">
              <b class="keyword">Open Vocabulary</b><br>
              Trained using a full English dictionary of nouns (no core or prioritized vocabulary)
            </div>
            <div class="adv-card">
              <b class="keyword">Scalable</b><br>
              Performance scales well with the performance of the underlying CLIP model
            </div>
            <div class="adv-card">
              <b class="keyword">Reliable</b><br>
              Achieves accurate <b>fine-grained</b> prediction scores up to <b>87.5%</b> on arbitrary images
            </div>
          </div>
        </div>
      </div>
    </div>

    <div class="carousel results-carousel">
      <div class="item">
        <div class="results-carousel-item">
          <img class="input-image" src="./static/images/demo/01_rhodesian_ridgeback.jpg" alt="A rhodesian ridgeback">
          <img class="output-pred" src="./static/images/demo/01_rhodesian_ridgeback.png" alt="NOVIC open vocabulary classification results">
        </div>
      </div>
      <div class="item">
        <div class="results-carousel-item">
          <img class="input-image" src="./static/images/demo/03_banknote.jpg" alt="A man holding a 20 euro banknote">
          <img class="output-pred" src="./static/images/demo/03_banknote.png" alt="NOVIC open vocabulary classification results">
        </div>
      </div>
      <div class="item">
        <div class="results-carousel-item">
          <img class="input-image" src="./static/images/demo/11_pedestrian_crossing.jpg" alt="A woman crossing a rainbow-styled pedestrian crossing">
          <img class="output-pred" src="./static/images/demo/11_pedestrian_crossing.png" alt="NOVIC open vocabulary classification results">
        </div>
      </div>
      <div class="item">
        <div class="results-carousel-item">
          <img class="input-image" src="./static/images/demo/13_hearing_aid.jpg" alt="A man holding a hearing aid close to his ear">
          <img class="output-pred" src="./static/images/demo/13_hearing_aid.png" alt="NOVIC open vocabulary classification results">
        </div>
      </div>
      <div class="item">
        <div class="results-carousel-item">
          <img class="input-image" src="./static/images/demo/15_hippopotamus.jpg" alt="A hippopotamus splashing through a river">
          <img class="output-pred" src="./static/images/demo/15_hippopotamus.png" alt="NOVIC open vocabulary classification results">
        </div>
      </div>
      <div class="item">
        <div class="results-carousel-item">
          <img class="input-image" src="./static/images/demo/02_daffodil.jpg" alt="A daffodil">
          <img class="output-pred" src="./static/images/demo/02_daffodil.png" alt="NOVIC open vocabulary classification results">
        </div>
      </div>
      <div class="item">
        <div class="results-carousel-item">
          <img class="input-image" src="./static/images/demo/08_spur.jpg" alt="A spur used in the context of horses">
          <img class="output-pred" src="./static/images/demo/08_spur.png" alt="NOVIC open vocabulary classification results">
        </div>
      </div>
      <div class="item">
        <div class="results-carousel-item">
          <img class="input-image" src="./static/images/demo/12_compression_bandage.jpg" alt="Two compression bandages">
          <img class="output-pred" src="./static/images/demo/12_compression_bandage.png" alt="NOVIC open vocabulary classification results">
        </div>
      </div>
    </div>
    <div class="carousel results-carousel">
      <div class="item">
        <div class="results-carousel-item">
          <img class="input-image" src="./static/images/demo/09_red_panda.jpg" alt="A red panda">
          <img class="output-pred" src="./static/images/demo/09_red_panda.png" alt="NOVIC open vocabulary classification results">
        </div>
      </div>
      <div class="item">
        <div class="results-carousel-item">
          <img class="input-image" src="./static/images/demo/07_church_aurora.jpg" alt="A church with a strong green aurora visible in the sky">
          <img class="output-pred" src="./static/images/demo/07_church_aurora.png" alt="NOVIC open vocabulary classification results">
        </div>
      </div>
      <div class="item">
        <div class="results-carousel-item">
          <img class="input-image" src="./static/images/demo/04_magic_lantern.jpg" alt="A magic lantern">
          <img class="output-pred" src="./static/images/demo/04_magic_lantern.png" alt="NOVIC open vocabulary classification results">
        </div>
      </div>
      <div class="item">
        <div class="results-carousel-item">
          <img class="input-image" src="./static/images/demo/06_armadillo.jpg" alt="An armadillo">
          <img class="output-pred" src="./static/images/demo/06_armadillo.png" alt="NOVIC open vocabulary classification results">
        </div>
      </div>
      <div class="item">
        <div class="results-carousel-item">
          <img class="input-image" src="./static/images/demo/05_roof_rack.jpg" alt="A roof rack mounted on top of a car">
          <img class="output-pred" src="./static/images/demo/05_roof_rack.png" alt="NOVIC open vocabulary classification results">
        </div>
      </div>
      <div class="item">
        <div class="results-carousel-item">
          <img class="input-image" src="./static/images/demo/10_bicycle_rack.jpg" alt="A bicycle rack at a curbside">
          <img class="output-pred" src="./static/images/demo/10_bicycle_rack.png" alt="NOVIC open vocabulary classification results">
        </div>
      </div>
      <div class="item">
        <div class="results-carousel-item">
          <img class="input-image" src="./static/images/demo/16_wreath.jpg" alt="A lamp post adorned with a wreath, located in Florida Key">
          <img class="output-pred" src="./static/images/demo/16_wreath.png" alt="NOVIC open vocabulary classification results">
        </div>
      </div>
      <div class="item">
        <div class="results-carousel-item">
          <img class="input-image" src="./static/images/demo/14_praying_mantis.jpg" alt="A praying mantis">
          <img class="output-pred" src="./static/images/demo/14_praying_mantis.png" alt="NOVIC open vocabulary classification results">
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title">Live Demo</h2>
        <div class="content has-text-justified">
          <p>
            You can try out NOVIC yourself on your <b>own images</b> using the <a href="https://huggingface.co/spaces/pallgeuer/novic" target="_blank">Hugging Face Spaces Live Demo</a>! Alternatively, you can easily get started inferencing NOVIC yourself on a local GPU, using the <a href="https://hub.docker.com/r/pallgeuer/novic">pre-built NOVIC Docker image</a> and the associated <a href="https://github.com/pallgeuer/novic#inference-novic-yourself">compact instructions</a>.
          </p>
          <div class="publication-image">
            <a href="https://huggingface.co/spaces/pallgeuer/novic" target="_blank">
              <img class="tight-bottom" src="./static/images/novic_spaces.png" alt="Screenshot of live NOVIC model demo on Hugging Face Spaces">
            </a>
          </div>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title">Approach</h2>
        <div class="content has-text-justified">
          <img src="./static/images/novic.png" alt="Model training and inference pipeline overview schematic">
          <p>
            <b>NOVIC</b> is an innovative u<b>N</b>constrained <b>O</b>pen <b>V</b>ocabulary <b>I</b>mage <b>C</b>lassifier that uses an autoregressive transformer to generatively output classification labels as language. Leveraging the extensive knowledge of CLIP models, NOVIC harnesses the embedding space to enable <b>zero-shot transfer from pure text to images</b>. Traditional CLIP models, despite their ability for open vocabulary classification, require an exhaustive prompt of potential class labels, restricting their application to images of known content or context. To address this, NOVIC uses an <b>object decoder</b> model that is trained on a large-scale 92M-target dataset of templated object noun sets and LLM-generated captions to always output the object noun in question. This effectively inverts the CLIP text encoder and allows textual object labels to be generated directly from image-derived embedding vectors, without requiring any a priori knowledge of the potential content of an image. NOVIC has been tested on a mix of manually and web-curated datasets, as well as standard image classification benchmarks, and achieves <b>fine-grained prompt-free prediction scores of up to 87.5%</b>, a strong result considering the model must work for <b>any conceivable image and without any contextual clues</b>.
          </p>
          <img src="./static/images/object_decoder.png" alt="Object decoder model architecture overview schematic">
          <p>
            At the heart of the NOVIC architecture is the <b>object decoder</b>, which effectively inverts the CLIP text encoder, and learns to map CLIP embeddings to object noun classification labels in the form of tokenized text. During training, a <b>synthetic text-only dataset</b> is used to train the object decoder to map the CLIP text embeddings corresponding to templated/generated captions to the underlying target object nouns. During inference, zero-shot transfer is used to map CLIP image embeddings (as opposed to text embeddings) to predicted object nouns. The ability of the object decoder to generalize from text embeddings to image embeddings is non-trivial, as there is a huge modality gap between the two types of embeddings (for all CLIP models), with the embeddings in fact occupying two completely disjoint areas of the embedding space, with much gap in-between.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title">Training Datasets</h2>
        <div class="content has-text-justified">
          <img src="./static/images/novic_datasets.png" alt="Example prompt-templated and LLM-generated caption-object text pairs" style="max-width: 450px;">
          <p>
            Despite being an <b>image classification model</b>, NOVIC is trained exclusively on synthetic textual data in the form of matching <b>caption-object pairs</b>. These are primarily generated using a multi-object prompt templating strategy based on the <a href="https://github.com/pallgeuer/object_noun_dictionary">Object Noun Dictionary</a>. The remaining caption-object pairs were one-time generated using an LLM based on the same dictionary of nouns. The complete data required for training is available on Papers With Code as the <a href="https://paperswithcode.com/dataset/novic-caption-object-data">NOVIC Caption-Object Data</a>.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title">Evaluation Datasets</h2>
        <div class="content has-text-justified">
          <img src="./static/images/ovic_datasets.jpg" alt="Example images from the OVIC datasets">
          <p>
            In order to properly evaluate a <b>totally novel generative image classification model</b> like NOVIC, three new open vocabulary datasets were constructed and annotated. These datasets are generic, i.e. not specific to NOVIC, and are referred to as the <a href="https://paperswithcode.com/dataset/ovic-datasets">OVIC Datasets</a>:
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-multiline is-variable is-2">
      <div class="column is-one-third">
        <div class="card">
          <header class="card-header">
            <p class="card-header-title is-justify-content-center">World (272 images)</p>
          </header>
          <div class="card-content">
            <div class="content has-text-centered">
              Mostly never-online photos from <strong>10 countries</strong> by <strong>12 people</strong>, including diverse, atypical, deceptive, and indirect objects.
            </div>
          </div>
          <footer class="card-footer">
            <a class="card-footer-item" href="https://www2.informatik.uni-hamburg.de/wtm/corpora/ovic_datasets/world_dataset.zip">Download&nbsp;<span class="icon"><i class="fas fa-download"></i></span></a>
          </footer>
        </div>
      </div>
      <div class="column is-one-third">
        <div class="card">
          <header class="card-header">
            <p class="card-header-title is-justify-content-center">Wiki (1000 images)</p>
          </header>
          <div class="card-content">
            <div class="content has-text-centered">
              Wikipedia article lead images that were sampled from a pool of 18K such scraped images.
            </div>
          </div>
          <footer class="card-footer">
            <a class="card-footer-item" href="https://www2.informatik.uni-hamburg.de/wtm/corpora/ovic_datasets/wiki_dataset.zip">Download&nbsp;<span class="icon"><i class="fas fa-download"></i></span></a>
          </footer>
        </div>
      </div>
      <div class="column is-one-third">
        <div class="card">
          <header class="card-header">
            <p class="card-header-title is-justify-content-center">Val3K (3000 images)</p>
          </header>
          <div class="card-content">
            <div class="content has-text-centered">
              Images from the ImageNet-1K validation set, sampled uniformly across the classes.
            </div>
          </div>
          <footer class="card-footer">
            <a class="card-footer-item" href="https://www2.informatik.uni-hamburg.de/wtm/corpora/ovic_datasets/val3k_dataset.zip">Download&nbsp;<span class="icon"><i class="fas fa-download"></i></span></a>
          </footer>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title">Presentation Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/vyR2QHUH9NY?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered" id="bibtex">
      <div class="column is-full-width">
        <h2 class="title">BibTeX</h2>
        <div class="content has-text-justified">
          <p>
            If you use this project in your research, please cite the <a href="https://openaccess.thecvf.com/content/WACV2025/html/Allgeuer_Unconstrained_Open_Vocabulary_Image_Classification_Zero-Shot_Transfer_from_Text_to_WACV_2025_paper.html">WACV 2025 paper</a>, possibly as well as the GitHub repository:
          </p>
          <pre><code>@InProceedings{allgeuer_novic_2025,
    author    = {Philipp Allgeuer and Kyra Ahrens and Stefan Wermter},
    title     = {Unconstrained Open Vocabulary Image Classification: {Z}ero-Shot Transfer from Text to Image via {CLIP} Inversion},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    year      = {2025},
}

@Misc{github_novic,
    title = {{NOVIC}: {U}nconstrained Open Vocabulary Image Classification},
    author = {Philipp Allgeuer and Kyra Ahrens},
    url = {https://github.com/pallgeuer/novic},
}</code></pre>
        </div>
      </div>
    </div>

  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link external-link" href="https://arxiv.org/abs/2407.11211">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link external-link" href="https://huggingface.co/spaces/pallgeuer/novic">
        <i class="fas fa-rocket"></i>
      </a>
      <a class="icon-link external-link" href="https://github.com/pallgeuer/novic">
        <i class="fab fa-github"></i>
      </a>
      <a class="icon-link external-link" href="https://paperswithcode.com/dataset/ovic-datasets">
        <i class="far fa-images"></i>
      </a>
      <a class="icon-link external-link" href="https://www.youtube.com/watch?v=vyR2QHUH9NY">
        <i class="fab fa-youtube"></i>
      </a>
      <a class="icon-link external-link" href="https://paperswithcode.com/paper/unconstrained-open-vocabulary-image">
        <i class="fas fa-file-code"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <div class="content has-text-centered">
            <p>
              The website template was adapted from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
